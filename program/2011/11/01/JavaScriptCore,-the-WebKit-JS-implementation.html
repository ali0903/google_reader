<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>JavaScriptCore, the WebKit JS implementation</title>
        <meta name="viewport" content="width=device-width">

        <!-- syntax highlighting CSS -->
        <link rel="stylesheet" href="/css/syntax.css">

        <!-- Custom CSS -->
        <link rel="stylesheet" href="/css/main.css">

    </head>
    <body>

        <div class="container">
          <div class="site">
            <div class="header">
              <h1 class="title"><a href="/">Your New Jekyll Site</a></h1>
              <a class="extra" href="/">home</a>
            </div>

                <h2>JavaScriptCore, the WebKit JS implementation</h2>
<p class="meta">01 Nov 2011</p>

<div class="post">
<h2>JavaScriptCore, the WebKit JS implementation</h2>

<h3>by</h3>

<h3>at 2011-10-31 20:23:00</h3>

<h3>original <a href="http://wingolog.org/archives/2011/10/28/javascriptcore-the-webkit-js-implementation">http://wingolog.org/archives/2011/10/28/javascriptcore-the-webkit-js-implementation</a></h3>

<div><div><p>My readers will know that I have recently had the pleasure of looking into the <a href="http://wingolog.org/tags/v8">V8</a> JavaScript implementation, from <a href="http://code.google.com/p/v8">Google</a>.  I'm part of a small group in <a href="http://www.igalia.com/">Igalia</a> doing compiler work, and it's clear that in addition to being lots of fun, JavaScript implementations are an important part of the compiler market today.</p><p>But V8 is not the only JS implementation in town.  Besides Mozilla's <a href="https://developer.mozilla.org/en/SpiderMonkey">SpiderMonkey</a>, which you probably know, there is another major Free Software JS implementation that you might not have even heard of, at least not by its proper name: JavaScriptCore.</p><p><b>jsc: js for webkit</b></p><p>JavaScriptCore (JSC) is the JavaScript implementation of the <a href="http://webkit.org/">WebKit</a> project.</p><p>In the beginning, JavaScriptCore was a simple tree-based interpreter<strike>, as Mozilla's SpiderMonkey was</strike>.  But then in June of 2008, a few intrepid hackers at Apple wrote a compiler and bytecode interpreter for JSC, threw away the tree-based interpreter, and called the thing <a href="http://www.webkit.org/blog/189/announcing-squirrelfish/">SquirrelFish</a>.  This was eventually marketed as "Nitro" inside Apple's products[0].</p><p>JSC's bytecode interpreter was great, and is still pretty interesting.  I'll go into some more details later in this article, because its structure conditions the rest of the implementation.</p><p>But let me continue here with this historical sketch by noting that later in 2008, the WebKit folks added inline caches, a regular expression JIT, and a simple method JIT, and then called the thing <a href="http://www.webkit.org/blog/214/introducing-squirrelfish-extreme/">SquirrelFish Extreme</a>.  Marketers called this Nitro Extreme.  (Still, the proper name of the engine is JavaScriptCore; <a href="http://en.wikipedia.org/wiki/JavaScript_engine">Wikipedia currently gets this one wrong.</a>)</p><p>One thing to note here is that the JSC folks were doing great, well-factored work.  It was so good that SpiderMonkey hackers at Mozilla adopted JSC's regexp JIT compiler and their native-code assembler directly.</p><p>As far as I can tell, for JSC, 2009 and 2010 were spent in "consolidation".  By that I mean that JSC had a JIT and a bytecode interpreter, and they wanted to maintain them both, and so there was a lot of refactoring and tweaking to make them interoperate.  This phase consolidated the SFX gains on x86 architectures, while also adding implementations for ARM and other architectures.</p><p>But with the release of V8's Crankshaft in late 2010, the JS performance bar had been lowered again (assuming it is a limbo), and so JSC folks started working on what they call their "DFG JIT" (DFG for "data-flow graph"), which aims be more like Crankshaft, basically.</p><p>It's possible to configure a JSC with all three engines: the interpreter, the simple method JIT, and the DFG JIT.  In that case there is tiered compilation between the three forms: initial parsing and compilation produces bytecode, that can be optimized with the method JIT, that can be optimized by the DFG JIT.  In practice, though, on most platforms the interpreter is not included, so that all code runs through the method JIT.  As far as I can tell, the DFG JIT is shipping in Mac OS X Lion's Safari browser, but it is not currently enabled on any platform other than 64-bit Mac.  (I am working on getting that fixed.)</p><p><b>a register vm</b></p><p>The interpreter has a number of interesting pieces, but it is important mostly for defining the format of bytecode.  Bytecode is effectively the high-level intermediate representation (IR) of JSC.</p><p>To put that into perspective, in V8, the high-level intermediate representation is the JS source code itself.  When V8 first sees a piece of code, it <i>pre-parses</i> it to raise early syntax errors.  Later when it needs to analyze the source code, either for the full-codegen compiler or for Hydrogen, it re-parses it to an AST, and then works on the AST.</p><p>In contrast, in JSC, when code is first seen, it is fully parsed to an AST and then that AST is compiled to bytecode.  After producing the bytecode, the source text isn't needed any more, and so it is forgotten.  The interpreter interprets the bytecode directly.  The simple method JIT compiles the bytecode directly.  The DFG JIT has to re-parse the bytecode into an SSA-style IR before optimizing and producing native code, which is a bit more expensive but worth it for hot code.</p><p>As you can see, bytecode is the common language spoken by all of JSC's engines, so it's important to understand it.</p><p>Before really getting into things, I should make an aside about terminology here.  Traditionally, at least in my limited experience, a virtual machine was considered to be a piece of software that interprets sequences of virtual instructions.  This would be in contrast to a "real" machine, that interprets sequences of "machine" or "native" instructions in hardware.</p><p>But these days things are more complicated.  A common statement a few years ago would be, "is JavaScript interpreted or compiled?"  This question is nonsensical, because "interpreted" or "compiled" are properties of implementations, not languages.  Furthermore the implementation can compile to bytecode, but then interpret that bytecode, as JSC used to do.</p><p>And in the end, if you compile all the bytecode that you see, where is the "virtual machine"?  V8 hackers still call themselves "virtual machine engineers", even as there is no interpreter in the V8 sources (not counting the ARM simulator; and what of a program run under qemu?).</p><p>All in all though, it is still fair to say that JavaScriptCore's high-level intermediate language is a sequence of virtual instructions for an abstract register machine, of which the interpreter and the simple method JIT are implementations.</p><p>When I say "register machine", I mean that in contrast to a "stack machine".  The difference is that in a register machine, all temporary values have names, and are stored in slots in the stack frame, whereas in a stack machine, temporary results are typically pushed on the stack, and most instructions take their operands by popping values off the stack.</p><p>(Incidentally, V8's full-codegen compiler operates on the AST in a stack-machine-like way.  Accurately modelling the state of the stack when switching from full-codegen to Crankshaft has been a source of many bugs in V8.)</p><p>Let me say that for an interpreter, I am totally convinced that register machines are the way to go.  I say this as a Guile co-maintainer, which has a stack VM.  Here are some reasons.</p><p>First, stack machines penalize named temporaries.  For example, consider the following code:</p><pre>(lambda (x)
  (* (+ x 2)
     (+ x 2)))
</pre><p>We could do common-subexpression elimination to optimize this:</p><pre>(lambda (x)
  (let ((y (+ x 2)))
    (* y y)))
</pre><p>But in a stack machine is this really a win?  Consider the sequence of instructions in the first case:</p><pre>; stack machine, unoptimized
0: local-ref 0      ; x
1: make-int8 2
2: add
3: local-ref 0      ; x
4: make-int8 2
5: add
6: mul
7: return
</pre><p>Contrast this to the instructions for the second case:</p><pre>; stack machine, optimized
0: local-ref 0      ; push x
1: make-int8 2      ; push 2
2: add              ; pop x and 2, add, and push sum
3: local-set 1      ; pop and set y
4: local-ref 1      ; push y
5: local-ref 1      ; push y
6: mul              ; pop y and y, multiply, and push product
7: return           ; pop and return
</pre><p>In this case we really didn't gain anything, because the storing values to locals and loading them back to the stack take up separate instructions, and in general the time spent in a procedure is linear in the number of instructions executed in the procedure.</p><p>In a register machine, on the other hand, things are easier, and CSE is definitely a win:</p><pre>0: add 1 0 0           ; add x to x and store in y
1: mul 2 1 1           ; multiply y and y and store in z
2: return 2            ; return z
</pre><p>In a register machine, there is no penalty to naming a value.  Using a register machine reduces the push/pop noise around the instructions that do the real work.</p><p>Also, because they include the names (or rather, locations) of their operands within the instruction, register machines also take fewer instructions to do the job.  This reduces dispatch cost.</p><p>In addition, with a register VM, you know the size of a call frame before going into it, so you can avoid overflow checks when pushing values in the function.  (Some stack machines also have this property, like the JVM.)</p><p>But the big advantage of targeting a register machine is that you can take advantage of traditional compiler optimizations like CSE and register allocation.  In this particular example, we have used three virtual registers, but in reality we only need one.  The resulting code is also closer to what real machines expect, and so is easier to JIT.</p><p>On the down side, instructions for a register machine typically occupy more memory than instructions for a stack machine.  This is particularly the case for JSC, in which the opcode and each of the operands takes up an entire machine word.  This was done to implement "direct threading", in which the opcodes are not indexes into jump tables, but actually are the addresses of the corresponding labels.  This might be an acceptable strategy for an implementation of JS that doesn't serialize bytecode out to disk, but for anything else the relocations are likely to make it a lose.  In fact I'm not sure that it's a win for JSC even, and perhaps the bloat was enough of a lose that the interpreter was turned off by default.</p><p>Stack frames for the interpreter consist of a six-word frame, the arguments to the procedure, and then the locals.  Calling a procedure reserves space for a stack frame and then pushes the arguments on the stack -- or rather, sets them to the last <var>n</var> + 6 registers in the stack frame -- then slides up the frame pointer.  For some reason in JSC the stack is called the "register file", and the frame pointer is the "register window".  Go figure; I suppose the names are as inscrutable as the "activation records" of the stack world.</p><p><b>jit: a jit, a method jit</b></p><p>I mention all these details about the interpreter and the stack (I mean, the register file), because they apply directly to the method JIT.  The simple method JIT (which has no name) does the exact same things that the bytecode interpreter does, but it does them via emitted machine instructions instead of interpreting virtual instructions.</p><p>There's not much to say here; jitting the code has the result you would expect, reducing dispatching overhead, while at the same time allowing some context-specific compilation, like when you add a constant integer to a variable.  This JIT is really quick-and-dirty though, so you don't get a lot of the visibility benefits traditionally associated with method JITs like what HotSpot's C1 or C2 currently have.  Granted, the register VM bytecode does allow for some important optimizations to happen, but JSC currently doesn't do very much in the way of optimizing bytecode, as far as I can tell.</p><p>Thinking more on the subject, I suspect that for Javascript, CSE isn't even possible unless you know the types, as a <tt>valueOf()</tt> callback could have side effects.</p><p><b>an interlude of snarky footnotes</b></p><p>Hello, reader!  This is a long article, and it's a bit dense.  I had some snarky footnotes that I enjoyed writing, but it felt wrong to put them at the end, so I thought it better to liven things up in the middle here.  The article continues in the next section.</p><p>0.  In case you didn't know, compilers are approximately 37% composed of marketing, and rebranding is one of the few things you can do to a compiler, marketing-wise, hence the name train:  SquirrelFish, Nitro, SFX, Nitro Extreme...[1] As in, in response to "I heard that Nitro is slow.", one hears, "Oh, man they totally fixed that in SquirrelFish Extreme.  It's blazingly fast![2]"</p><p>1.  I don't mean to pick on JSC folks here.  V8 definitely has this too, with their "big reveals".  Firefox people continue to do this for some reason (SpiderMonkey, TraceMonkey, JaegerMonkey, IonMonkey).  I expect that even they have forgotten the reason at this point.  In fact the JSC marketing has lately been more notable in its absence, resulting in a dearth of useful communication.  At this point, in response to "Oh, man they totally are doing a great job in JavaScriptCore", you're most likely to hear, "JavaScriptCore?  Never heard of it.  Kids these days hack the darndest things."</p><p>2.  This is the other implement in the marketer's toolbox: "blazingly fast".  It means, "I know that you don't understand anything I'm saying, but I would like for you to repeat this phrase to your colleagues please."  As in, "LLVM does advanced TBAA on the SSA IR, allowing CSE and LICM while propagating copies to enable SIMD loop vectorization.  It is blazingly fast."</p><p><b>dfg: a new crankshaft for jsc?</b></p><p>JavaScriptCore's data flow graph (DFG) JIT is work by Gavin Barraclough and Filip Pizlo to enable speculative optimizations for JSC.  For example, if you see the following code in JS:</p><pre>a[i++] = 0.7*x;
</pre><p>then <tt>a</tt> is probably an array of floating-point numbers, and <tt>i</tt> is probably an integer.  To get great performance, you want to use native array and integer operations, so you speculatively compile a version of your code that makes these assumptions.  If the assumptions don't work out, then you bail out and try again with the normal method JIT.</p><p>The fact that the interpreter and simple method JIT have a clear semantic model in the form of bytecode execution makes it easy to bail out: you just reconstruct the state of the virtual registers and register window, then jump back into the code.  (V8 calls this process "deoptimization"; the DFG calls it "speculation failure".)</p><p>You can go the other way as well, switching from the simple JIT to the optimized DFG JIT, using <a href="http://wingolog.org/archives/2011/06/20/on-stack-replacement-in-v8">on-stack replacement</a>.  The DFG JIT does do OSR.  I hear that it's needed if you want to win <a href="http://krakenbenchmark.mozilla.org/">Kraken</a>, which puts you in lots of tight loops that you need to be able to optimize without relying on being able to switch to optimized code only on function re-entry.</p><p>When the DFG JIT is enabled, the interpreter (if present) and the simple method JIT are augmented with profiling information, to record what types flow through the various parts of the code.  If a loop is executed a lot (currently more than 1000 times), or a function is called a lot (currently about 70 times), the DFG JIT kicks in.  It parses the bytecode of a function into an SSA-like representation, doing inlining and collecting type feedback along the way.  This probably sounds <a href="http://wingolog.org/archives/2011/08/02/a-closer-look-at-crankshaft-v8s-optimizing-compiler">very familiar to my readers</a>.</p><p>The difference between JSC and Crankshaft here is that Crankshaft parses out type feedback from the inline caches directly, instead of relying on in-code instrumentation.  I think Crankshaft's approach is a bit more elegant, but it is prone to lossage when GC blows the caches away, and in any case either way gets the job done.</p><p>I mentioned inlining before, but I want to make sure that you noticed it: the DFG JIT does do inlining, and does so at parse-time, like HotSpot does.  The type profiling (they call it "value profiling") combined with some cheap static analysis also allows the DFG to unbox int32 and double-precision values.</p><p>One thing that the DFG JIT doesn't do, currently, is much in the way of code motion.  It does do some dead-code elimination and common-subexpression elimination, and as I mentioned before, you need the DFG's value profiles in order to be able to do this correctly.  But it does not do loop-invariant code motion.</p><p>Also, the DFG's register allocator is not as good as Crankshaft's, yet.  It is hampered in this regard by the JSC assembler that I praised earlier; while indeed a well-factored, robust piece of code, JSC's assembler has a two-address interface instead of a three-address interface.  This means that instead of having methods like <tt>add(dest, op1, op2)</tt>, it has methods like <tt>add(op1, op2)</tt>, where the operation implicitly stores its result in its first operand.  Though it does correspond to the x86 instruction set, this sort of interface is not great for systems where you have more registers (like on x86-64), and forces the compiler to shuffle registers around a lot.</p><p>The counter-based optimization triggers do require some code to run that isn't strictly necessary for the computation of the results, but this strategy does have the nice property that the DFG performance is fairly predictable, and measurable.  Crankshaft, on the other hand, being triggered by a statistical profiler, has <a href="http://arewefastyet.com/?a=b&amp;machine=8">statistically variable performance</a>.</p><p>And speaking of performance, <a href="http://arewefastyet.com/?a=b&amp;machine=8">AWFY on the mac</a> is really where it's at for JSC right now.  Since the DFG is only enabled by default on recent Mac OS 64-bit builds, you need to be sure you're benchmarking the right thing.</p><p>Looking at the results, I think we can say that JSC's performance on the V8 benchmark is really good.  Also it's interesting to see JSC beat V8 on SunSpider.  Of course, there are lots of quibbles to be had as to the validity of the various benchmarks, and it's also clear that V8 is the fastest right now once it has time to warm up.  But I think we can say that JSC is doing good work right now, and getting better over time.</p><p><b>future</b></p><p>So that's JavaScriptCore.  The team -- three people, really -- is mostly focusing on getting the DFG JIT working well right now, and I suspect they'll be on that for a few months.  But we should at least get to the point where the DFG JIT is working and enabled by default on free systems within a week or two.</p><p>The one other thing that's in the works for JSC is a new generational garbage collector.  This is progressing, but slowly.  There are stubs in the code for card-marking write barriers, but currently there is no such GC implementation, as far as I can tell.  I suspect that someone has a patch they're cooking in private; we'll see.  At least JSC does have a <a href="http://trac.webkit.org/browser/trunk/Source/JavaScriptCore/heap/Handle.h">Handle</a> API, unlike SpiderMonkey.</p><p><b>conclusion</b></p><p>So, yes, in summary, JavaScriptCore is a fine JS implementation.  Besides being a correct implementation for real-world JS -- something that is already saying quite a lot -- it also has good startup speed, is fairly robust, and is working on getting an optimizing compiler.  There's work to do on it, as with all JS implementations, but it's doing fine.</p><p>Thanks for reading, if you got this far, though I understand if you skipped some parts in the middle.  Comments and corrections are most welcome, from those of you that actually read all the way through, of course :).  Happy hacking!</p></div></div>


</div>


            <div class="footer">
              <div class="contact">
                <p>
                  Your Name<br />
                  What You Are<br />
                  your@email.com
                </p>
              </div>
              <div class="contact">
                <p>
                  <a href="http://github.com/yourusername/">github.com/yourusername</a><br />
                  <a href="http://twitter.com/yourusername/">twitter.com/yourusername</a><br />
                </p>
              </div>
            </div>
          </div>
        </div> <!-- /container -->

    </body>
</html>
